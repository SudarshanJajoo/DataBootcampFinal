# -*- coding: utf-8 -*-
"""SudarshanJajoo_FinalProject_Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gLWF6OI2wvZviRgHNg8LV7NfbJpOEWF_

# Predicting Housing Sale prices in Iowa

## Modeling & Interpretations
"""

import kagglehub

path = kagglehub.dataset_download("shashanknecrothapa/ames-housing-dataset")

path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import cross_val_score
from sklearn.inspection import permutation_importance
import os

dataset_path = '/root/.cache/kagglehub/datasets/shashanknecrothapa/ames-housing-dataset/versions/1'
print(os.listdir(dataset_path))

file_path = '/root/.cache/kagglehub/datasets/shashanknecrothapa/ames-housing-dataset/versions/1/AmesHousing.csv'
df = pd.read_csv(file_path)
df.head()

"""### Baseline Model

To establish a baseline for performance comparison, I created a simple model that predicts the average sale price across all houses in the dataset. This  approach assumes that all houses sell for the same price, equal to the dataset's mean sale price. The resulting mean squared error (MSE) serves as a benchmark to assess the improvements made by more sophisticated models.
"""

# Set up baseline model using mean SalePrice
y = df['SalePrice']  # Target variable
baseline_preds = np.ones(len(y)) * y.mean()
mse = mean_squared_error(y, baseline_preds)

print(f"Baseline MSE: {mse}")

"""The high MSE indicates that using the mean as a predictor results in substantial errors, underscoring the dataset's complexity and the variability in sale prices.

### Multiple Regression Model

In the updated approach, I refined the feature selection to include Garage Cars, Garage Area, Total Bsmt SF, 1st Flr SF, Overall Qual, and Neighborhood. These variables were chosen based on their relevance to housing prices, encompassing physical attributes, quality, and location factors.

I split the dataset into training and testing sets, ensuring an 80/20 split for evaluation. By building a multiple linear regression model, I captured the influence of each independent variable while accounting for their combined effects on sale prices.

By incorporating categorical and numerical features, this updated model is designed to handle the complexity of housing price prediction while improving upon the baseline model's performance.
"""

# Prepare features and target
X = df[['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual', 'Neighborhood']]
y = df['SalePrice']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)
    ]
)

# Create main pipeline
linear_pipe = Pipeline(
    steps=[
        ('preprocessor', preprocessor),
        ('regressor', LinearRegression())
    ]
)

# Fit the pipeline
linear_pipe.fit(X_train, y_train)

# Predict
linear_pred = linear_pipe.predict(X_test)

# Evaluate the model
def evaluate_model(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"{model_name} Results:")
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R² Score: {r2:.4f}")

evaluate_model(y_test, linear_pred, "Linear Regression")

# Extract and display feature importance
def analyze_features(model, feature_names):
    # Get coefficients for numerical and one-hot encoded features
    coefficients = model.named_steps['regressor'].coef_
    feature_names_full = model.named_steps['preprocessor'].transformers_[0][2] + \
                         list(model.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out())

    importance_df = pd.DataFrame({
        'Feature': feature_names_full,
        'Importance': coefficients
    })
    return importance_df.sort_values('Importance', ascending=False)

# Get feature importance
feature_importance = analyze_features(linear_pipe, X.columns)
print("Feature Importance:")
print(feature_importance)

# Calculate MSE using the linear_pipe
train_predictions = linear_pipe.predict(X_train)
train_mse = mean_squared_error(y_train, train_predictions)

test_predictions = linear_pipe.predict(X_test)
test_mse = mean_squared_error(y_test, test_predictions)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")

# Extract the y-intercept
y_intercept = linear_pipe.named_steps['regressor'].intercept_
print(f"Y-intercept: {y_intercept:.2f}")

# Add cross-validation
cv_scores = cross_val_score(linear_pipe, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# Visualize predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Linear Regression: Predicted vs Actual House Prices')
plt.tight_layout()
plt.show()

"""The multiple regression model developed demonstrates robust predictive performance for housing prices in the Ames dataset. Achieving an R² score of 0.7936, the model explains approximately 79% of the variance in housing prices, signifying a substantial improvement over previous iterations. This strong R² value highlights the effectiveness of the selected features in capturing key determinants of property values.

The feature importance analysis sheds light on the drivers of housing prices. Overall Quality emerged as the most significant predictor, underlining the critical role of construction and finishing quality. Gr Liv Area followed as the second most influential feature, reflecting the importance of total living space in valuation. Meanwhile, Year Built ranked third, emphasizing the value associated with modern or newer constructions. Features such as Total Bsmt SF and Lot Area showed lower relative importance but still contributed to refining predictions. These results align with expectations in real estate markets, where quality and usable space often dominate pricing considerations.

The visualization of predicted versus actual prices supports these findings. The scatterplot shows a strong linear relationship between predictions and actual values, with most data points clustering near the diagonal, indicating accurate predictions. However, there is some deviation, particularly at higher price points, where the model tends to slightly underpredict or overpredict. This discrepancy might stem from outliers or unaccounted variables influencing premium properties.

Performance metrics further validate the model's reliability. The training MSE of 1,381,475,858.92 and testing MSE of 1,655,198,876.59 indicate a modest gap, suggesting the model generalizes well to unseen data. Cross-validation scores, averaging 0.7726, reinforce this consistency across different data splits, with scores ranging from 0.7024 to 0.8143, confirming the model's robustness.

### K - Nearest Neighbour Regression Model

I implemented a K-Nearest Neighbors (KNN) regression model to capture the influence of neighborhood effects and local market dynamics on housing prices. Given that houses with similar features, such as size, neighborhood, and quality, often cluster together in similar price ranges, KNN is well-suited to predict housing prices by leveraging this feature similarity.

This approach is particularly advantageous for the Ames housing dataset, as it allows the model to focus on local price variations and adapt predictions to neighborhood-specific characteristics. By analyzing housing features such as Garage Area, Total Bsmt SF, Lot Area, Overall Quality, and Year Built, the model effectively captures the price determinants associated with individual properties and their immediate surroundings.

The KNN implementation integrates cross-validation and hyperparameter tuning to optimize the number of neighbors and weight functions, further enhancing predictive performance and robustness. This refined methodology ensures the model's ability to deliver accurate predictions across diverse housing segments, reflecting both local trends and broader market patterns.
"""

# Prepare features and target
X = df[['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual', 'Neighborhood']]
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
transformer = make_column_transformer(
    (Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute categorical columns
        ('encoder', OneHotEncoder(drop='first', sparse_output=False))
    ]), cat_col),
    (Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),  # Impute numerical columns
        ('scaler', StandardScaler())
    ]), numeric_cols),
    remainder='passthrough'
)

# Create preprocessing pipeline
preprocessor = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Create KNN pipeline
knn_pipe = Pipeline([
    ('preprocessor', transformer),
    ('regressor', KNeighborsRegressor())
])

# Define parameter grid
param_grid = {
    'regressor__n_neighbors': [3, 5, 7, 9, 11, 25],
    'regressor__weights': ['uniform', 'distance']
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(knn_pipe, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Evaluate the model on test data
best_knn_model = grid_search.best_estimator_
knn_predictions = best_knn_model.predict(X_test)

# Calculate performance metrics
knn_mse = mean_squared_error(y_test, knn_predictions)
knn_rmse = np.sqrt(knn_mse)
knn_r2 = r2_score(y_test, knn_predictions)

print(f"KNN Best Parameters: {grid_search.best_params_}")
print(f"KNN MSE: {knn_mse:.2f}")
print(f"KNN RMSE: {knn_rmse:.2f}")
print(f"KNN R² Score: {knn_r2:.4f}")

# Perform 5-fold cross-validation
cv_scores = cross_val_score(knn_pipe, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

from sklearn.inspection import permutation_importance

# Calculate permutation importance
r = permutation_importance(best_knn_model, X_test, y_test, n_repeats=10, random_state=42)

# Extract feature names
categorical_features = ['Neighborhood']  # Represent Neighborhood as a single feature
feature_names = categorical_features + numeric_cols

# Aggregate importance for Neighborhood
neighborhood_indices = [i for i, name in enumerate(feature_names) if 'Neighborhood' in name]
neighborhood_importance = r['importances_mean'][neighborhood_indices].sum()

# Combine Neighborhood into one feature
importance_values = list(r['importances_mean'])
importance_values[len(categorical_features) - 1] = neighborhood_importance

# Create DataFrame of feature importances
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance_values
}).sort_values('Importance', ascending=False)

# Display feature importance
print("\nFeature Importance:")
print(feature_importance)

# Predict on the test set
test_predictions = best_knn_model.predict(X_test)

# Visualize predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('KNN: Predicted vs Actual House Prices')
plt.tight_layout()
plt.show()

"""The K-Nearest Neighbors (KNN) regression model shows a strong correlation between predicted and actual house prices in the Ames dataset, as illustrated in the graph. Most data points align closely with the red diagonal line, representing perfect predictions. This indicates that the model effectively captures the relationships between the selected features and housing prices, particularly for mid-range properties.

The model performs exceptionally well for properties within the mid-price range, where points are densely clustered around the diagonal. This suggests that the KNN model generalizes well for the majority of houses in the dataset, likely due to their greater representation in the training data. However, for higher-priced houses, deviations from the diagonal line are more apparent. Predicted prices for these properties often fall short of actual prices, reflecting potential difficulties in capturing the unique characteristics of high-value homes. These deviations could result from the limited number of training samples in this price range or the absence of features specific to luxury homes.

The minimal spread of residuals across the price range demonstrates the robustness of the model, avoiding significant overfitting or underfitting. The alignment of points across most price levels also highlights the effectiveness of the selected features, including Garage Area, Total Bsmt SF, Lot Area, Overall Quality, and Year Built. These features play a crucial role in determining housing prices, and their inclusion contributes significantly to the model's predictive power.

### Decision Tree Regression Model

I chose to implement a Decision Tree regression model for the Ames housing dataset because decision trees are highly effective at capturing non-linear relationships in data while providing valuable interpretability. Unlike the KNN model, which relies on feature similarity to identify local price patterns, decision trees create a hierarchical structure of decision rules that segment houses into different price brackets. This makes them particularly suitable for real estate price prediction, where housing prices often transition sharply at specific feature thresholds, such as living area, year built, or basement size.

The decision tree model automatically identifies important feature split points and thresholds, making it well-suited for understanding the nuanced interactions between housing characteristics and their influence on pricing. For example, the model can reveal price patterns that shift based on changes in overall quality or square footage. Additionally, its inherent interpretability provides clear and transparent insights into how features drive price predictions, offering practical value for real estate professionals, homeowners, and market analysts alike. By visualizing the tree, we gain a deeper understanding of the underlying structure of the data, enabling more informed decision-making and actionable insights.
"""

# Prepare features and target
X = df[['Neighborhood', 'Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']]
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']  # Define 'Neighborhood' as categorical
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
transformer = make_column_transformer(
    (OneHotEncoder(drop='first', sparse_output=False), cat_col),
    (StandardScaler(), numeric_cols),
    remainder='passthrough'
)

# Create pipeline for random forest model
pipe = Pipeline([
    ('encode', transformer),
    ('model', RandomForestRegressor(random_state=42))
])

"""The graph illustrates the relationship between the maximum depth of a decision tree and the mean squared error (MSE) for both training and testing datasets. As the tree depth increases, the model becomes more complex, capturing finer details of the data. However, this complexity introduces trade-offs between the model's performance on the training set and its generalizability to unseen data, as reflected in the test set error.

In the early stages, as the tree depth increases from shallow values (e.g., 1 to 5), there is a significant reduction in both training and testing errors. This indicates that the model is becoming better at capturing the underlying patterns in the data, leading to improved predictions for both datasets. During this phase, the decision tree strikes a balance between simplicity and accuracy, effectively generalizing to new data.

As the tree depth continues to grow beyond a certain point (around depth 7–10), the training error approaches zero, signifying that the model has memorized the training data and is no longer making errors. However, the test error stops decreasing and begins to fluctuate, indicating overfitting. Overfitting occurs when the model captures noise and overly specific details in the training data, which do not generalize well to new data points. The divergence between the training and testing errors in this range demonstrates the reduced ability of the model to generalize.

At very high depths, the test error stabilizes or increases slightly, reinforcing the conclusion that the model is too complex for effective generalization. This graph highlights the importance of selecting an optimal tree depth (around depth 7 in this case), where the test error is minimized without overfitting. Striking this balance ensures the model retains its predictive power on new data while avoiding unnecessary complexity.
"""

# Find the optimal depth from test scores
test_scores = []
for depth in range(1, 11):  # Test depths from 1 to 10
    dtree = DecisionTreeRegressor(max_depth=depth, random_state=42)
    dtree.fit(X_train_encoded, y_train)
    test_scores.append(mean_squared_error(y_test, dtree.predict(X_test_encoded)))

optimal_depth = test_scores.index(min(test_scores)) + 1
print(f"Optimal Depth: {optimal_depth}")

# Train the decision tree with the optimal depth
dtree = DecisionTreeRegressor(max_depth=optimal_depth, random_state=42)
dtree.fit(X_train_encoded, y_train)

# Train the decision tree with a smaller depth
dtree = DecisionTreeRegressor(max_depth=4, random_state=42)
dtree.fit(X_train_encoded, y_train)

# Plot the decision tree
plt.figure(figsize=(20, 8))
plot_tree(
    dtree,
    feature_names=encoder.get_feature_names_out(),
    filled=True,
    fontsize=12
)
plt.title("Decision Tree (Max Depth = 4)", fontsize=16)
plt.tight_layout()
plt.show()

# Calculate MSE for both sets
train_preds = dtree.predict(X_train_encoded)
test_preds = dtree.predict(X_test_encoded)

train_mse = mean_squared_error(y_train, train_preds)
test_mse = mean_squared_error(y_test, test_preds)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")

# Calculate additional metrics
train_r2 = r2_score(y_train, train_preds)
test_r2 = r2_score(y_test, test_preds)
rmse = np.sqrt(test_mse)

print(f"Training R²: {train_r2:.4f}")
print(f"Testing R²: {test_r2:.4f}")
print(f"RMSE: {rmse:.2f}")

# Perform cross-validation
cv_scores = cross_val_score(dtree, X_train_encoded, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# Calculate feature importance using permutation
r = permutation_importance(dtree, X_test_encoded, y_test, n_repeats=10, random_state=42)

# Create a DataFrame for feature importances
importance_df = pd.DataFrame({
    'Feature': encoder.get_feature_names_out().tolist(),
    'Importance': r['importances_mean']
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:")
print(importance_df)

# Plot residuals
residuals = y_test - test_predictions
plt.figure(figsize=(10, 6))
plt.scatter(test_predictions, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

"""The updated decision tree model performance demonstrates interesting nuances in its predictive capabilities. The model achieves a training R² score of 0.734 and a testing R² score of 0.711, showing a strong balance between fitting the training data and generalizing to unseen data. These results indicate the decision tree effectively captures underlying patterns in the Ames housing dataset, though with slightly less predictive accuracy than more flexible models like KNN.

The residual plot highlights key insights into the model's strengths and limitations. While the residuals are centered around zero, suggesting no significant bias in predictions, a notable pattern emerges: errors increase for higher-priced homes. This heteroscedasticity indicates that the decision tree model struggles to accurately predict prices for luxury homes, possibly due to the binary splitting structure that limits its ability to capture more complex price relationships.

Feature importance analysis reaffirms the critical role of Overall Quality and Gr Liv Area in price determination, with these features dominating the top splits in the tree. However, the residual plot and decision tree structure suggest that the model's simplicity might come at the cost of overlooking subtle influences from less dominant features, such as Year Built and Total Basement SF, which were influential in previous models.

Despite these limitations, the decision tree's interpretability remains a key advantage. Its hierarchical decision-making process provides clear insights into how housing attributes like quality and living area drive pricing. For real estate professionals and stakeholders, this clarity makes the decision tree an invaluable tool for understanding price segmentation and trends, even if it lacks the precision of more flexible models.

### Random Forest Regression Model

I chose to implement a Random Forest regression model for the Ames housing dataset due to its robustness and versatility in handling complex relationships and feature interactions. Unlike single decision trees, which are prone to overfitting, Random Forests aggregate predictions from multiple decision trees, creating an ensemble model that reduces variance while maintaining the interpretability of decision tree-based models. This ensemble approach allows the model to capture non-linear patterns and interactions between housing features, which are crucial in understanding housing price dynamics.

Random Forests are particularly well-suited for this dataset as they excel in managing high-dimensional data, handling both categorical and continuous variables seamlessly. By randomly selecting subsets of features and samples at each split, the model avoids over-reliance on any single feature or data point, ensuring more generalized predictions. This is especially valuable in real estate price prediction, where factors like quality, size, location, and year built interact in complex, non-linear ways.

Additionally, the model provides feature importance metrics, offering insights into the most influential predictors in determining housing prices. This transparency not only enhances interpretability but also helps prioritize key attributes that significantly impact pricing strategies. Given its balance between predictive accuracy, feature interpretability, and robustness to overfitting, Random Forest is a powerful model for addressing the challenges of housing price prediction in the Ames dataset.
"""

# Prepare features and target
X = df[['Year Built', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']]  # Updated features
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Year Built']
numeric_cols = ['Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
transformer = make_column_transformer(
    (OneHotEncoder(drop='first', sparse_output=False), cat_col),
    (StandardScaler(), numeric_cols),
    remainder='passthrough'
)

# Create pipeline for random forest model
pipe = Pipeline([
    ('encode', transformer),
    ('model', RandomForestRegressor(random_state=42))
])

# Define grid of hyperparameters
param_grid = {
    'model__n_estimators': [50, 100, 150, 200],
    'model__max_depth': [3, 4, 5, 6, 10]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get best parameters
print("Best parameters:", grid_search.best_params_)

# Use the best model from grid search
forest = grid_search.best_estimator_

# Make predictions
train_preds = forest.predict(X_train)
test_preds = forest.predict(X_test)

# Calculate performance metrics
train_mse = mean_squared_error(y_train, train_preds)
test_mse = mean_squared_error(y_test, test_preds)
r2_score_val = r2_score(y_test, test_preds)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")
print(f"R2 Score: {r2_score_val:.4f}")

# Calculate feature importance using permutation importance
r = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42)
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': r['importances_mean']
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
print(importance_df)

# Cross-validation scores display
cv_scores = cross_val_score(forest, X_train, y_train, cv=5)
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# First make predictions
test_predictions = forest.predict(X_test)

# Then create visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Random Forest: Predicted vs Actual House Prices')
plt.show()

"""The scatter plot above visualizes the performance of the Random Forest regression model by comparing predicted house prices against their actual values. The red dashed line represents the ideal scenario where predictions perfectly match the actual values, and data points lie on the line.

In this plot, most predictions cluster closely around the diagonal line, particularly for houses in the mid-price range. This indicates that the Random Forest model is performing well for properties within the average pricing brackets. However, the dispersion of points increases at higher price ranges, showing that the model struggles slightly with more expensive homes, where residuals (errors) are larger. This behavior could be due to the limited representation of higher-priced properties in the dataset, leading the model to generalize less effectively for these instances.

Additionally, the plot shows no obvious systematic bias in the predictions, as the spread of points is relatively even across the price range. This suggests that the model does not consistently overestimate or underestimate prices at specific intervals.

Overall, the Random Forest model demonstrates strong predictive accuracy, as evidenced by the tight clustering around the diagonal for most points. While it underperforms slightly for luxury properties, its ability to capture general price trends across the dataset confirms its utility as a robust and reliable model for predicting housing prices. This analysis aligns with the model's capability to aggregate decisions from multiple trees, reducing variance and improving overall predictive performance.
"""