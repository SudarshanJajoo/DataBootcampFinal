# -*- coding: utf-8 -*-
"""SudarshanJajoo_FinalProject

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbEptAszwjC6jfXoHm1Kk2yRyWYP7aMb

# Predicting Housing Sale prices in Iowa

# **Database Description**
"""

import kagglehub

path = kagglehub.dataset_download("shashanknecrothapa/ames-housing-dataset")

path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import cross_val_score
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import FunctionTransformer
import os

dataset_path = '/root/.cache/kagglehub/datasets/shashanknecrothapa/ames-housing-dataset/versions/1'
print(os.listdir(dataset_path))

"""## Preliminary Examination"""

file_path = '/root/.cache/kagglehub/datasets/shashanknecrothapa/ames-housing-dataset/versions/1/AmesHousing.csv'
df = pd.read_csv(file_path)
df.head()

print(df.info())
print("\nMissing Values:\n", df.isnull().sum()[df.isnull().sum() > 0])

df.columns

"""The average sale price is approximately $180,796, with values ranging from $12,789 to $755,000, reflecting a broad spectrum of property types. Homes in the dataset were constructed between 1872 and 2010, with the average year of construction being 1971, and many underwent remodeling or additions around 1984. Lot characteristics reveal an average lot area of 10,147 square feet and a mean lot frontage of 69.22 feet, highlighting the diversity in property sizes. Additional features include average wood deck areas of 93.75 square feet, open porches of 47.53 square feet, and enclosed porches of 23.01 square feet, providing further context on property amenities.

Correlation analysis reveals that Overall Quality (0.799), Above Ground Living Area (0.707), and Garage Capacity (0.648) are the strongest predictors of sale prices. These features emphasize the importance of material and finish quality, living space, and vehicle storage in determining property value. Basement area (Total Bsmt SF) also demonstrates a strong positive correlation (0.632). Conversely, some features, like enclosed porches (-0.129) and kitchens above grade (-0.120), show slight negative correlations with sale prices, suggesting they may detract from a home's overall appeal. Seasonal and luxury features, such as pool areas (0.068) and screen porches (0.112), exhibit weak correlations, indicating a limited impact on pricing in this dataset.

The dataset captures homes sold during a period of significant market fluctuation, with most transactions occurring between 2007 and 2008. This temporal context provides an opportunity to explore trends in property valuation within a mid-sized housing market during this timeframe. However, extreme outliers in features such as sale price and lot area highlight the need for careful preprocessing to ensure robust analysis.
"""

min_values = df[['SalePrice', 'Gr Liv Area', 'Lot Area', 'Garage Area', '1st Flr SF', 'Yr Sold']].min()
max_values = df[['SalePrice', 'Gr Liv Area', 'Lot Area', 'Garage Area', '1st Flr SF', 'Yr Sold']].max()

min_max_values = pd.DataFrame({'Min': min_values, 'Max': max_values})
print(min_max_values)

numerical_features = ['Lot Area', 'Overall Qual', '1st Flr SF',
                     'Total Bsmt SF', 'Garage Area', 'SalePrice']

plt.figure(figsize=(12, 6))
sns.boxplot(data=df[numerical_features])
plt.xticks(rotation=45)
plt.title('Distribution of Key Numerical Features')
plt.show()

"""The box plot above visualizes the distribution of six key numerical features in the Ames Housing dataset, offering significant insights into property characteristics. SalePrice displays the widest range, spanning from approximately $12,000 to over $700,000, with a median value around $160,000. The presence of numerous high-value outliers reflects the inclusion of luxury homes or properties with unique features. The skewness of SalePrice toward higher values indicates that while most properties cluster around the median, a smaller group of luxury homes significantly elevates the upper range of the distribution.

Lot Area exhibits substantial variability, with several extreme outliers exceeding 150,000 square feet. Despite these outliers, the majority of properties have lot sizes concentrated between 7,500 and 11,000 square feet, aligning with typical suburban standards. The presence of these outliers suggests that a small number of homes with exceptionally large plots of land contribute to the variability, which could potentially skew predictive models if not accounted for during preprocessing.

Overall Quality shows a more compact and consistent distribution, as it is measured on a scale from 1 to 10. Most properties cluster around a median quality rating of 6, indicating moderate construction quality. This narrow distribution suggests that the majority of homes share similar building standards, with fewer homes at the extreme ends of low or high quality.

1st Floor Square Footage (1st Flr SF) and Total Basement Square Footage (Total Bsmt SF) show moderately compact distributions, with a smaller number of high-value outliers. These features are closely tied to the usable living space of a property, and their relatively low variability implies that most properties have comparable living and basement areas, except for a few large or luxury homes.

Garage Area also demonstrates a consistent distribution, with limited outliers representing homes with particularly large garage spaces. Most properties appear to have garage areas that fall within a standard range, reflecting typical residential needs for storage and vehicle space.

The visualization highlights the diversity of features within the dataset while underscoring significant variability in some key attributes, particularly Lot Area and SalePrice. These features, due to their outliers and skewed distributions, could have a strong impact on predictive models if not properly addressed through preprocessing steps like scaling or outlier treatment. Such variability must be considered to ensure the robustness and accuracy of housing price predictions.
"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='SalePrice', kde=True)
plt.title('Distribution of House Prices')
plt.xlabel('Sale Price ($)')
plt.axvline(df['SalePrice'].mean(), color='red', linestyle='--', label='Mean')
plt.axvline(df['SalePrice'].median(), color='green', linestyle='--', label='Median')
plt.legend()
plt.show()

"""The histogram above illustrates the distribution of house prices in the dataset, providing valuable insights into the pricing trends of residential properties. The distribution is right-skewed, with most houses priced in the lower range while a smaller number of properties extend into higher price brackets. The majority of homes fall within the $100,000 to $250,000 range, which represents the peak of the distribution. This price range likely reflects typical residential properties with moderate features and quality, which cater to the average buyer in the Ames housing market.

The mean and median of the sale prices are indicated by dashed lines on the graph, with the red line representing the mean and the green line representing the median. The mean is slightly higher than the median, which aligns with the right-skewed nature of the data. This skewness suggests that the presence of high-value outliers, such as luxury homes priced above $500,000, pulls the mean upwards. These high-end properties, though fewer in number, have a disproportionate influence on the average sale price, causing it to exceed the median.

The long tail on the right-hand side of the distribution highlights the presence of outliers and high-value properties. These homes, priced between $300,000 and $750,000, likely include unique features such as larger lot areas, superior construction quality, and additional amenities, which differentiate them from the majority of houses. However, their low frequency relative to the overall dataset emphasizes that such properties are not representative of the typical housing market in Ames.

The median, being less sensitive to outliers, provides a better measure of central tendency for this skewed distribution. It indicates that most properties in the dataset are moderately priced, catering to buyers seeking affordable or mid-range homes. The histogram’s shape underscores the importance of handling outliers appropriately in predictive models, as failing to account for the skewness could distort the accuracy of predictions.
"""

# Create a correlation heatmap with adjusted size
plt.figure(figsize=(14, 10))  # Increase the figure size to reduce clutter
sns.heatmap(
    df[numerical_features].corr(),
    annot=True,
    cmap='coolwarm',
    center=0,
    fmt='.2f',  # Limit decimal points for annotations to two
    annot_kws={"size": 8}  # Reduce annotation font size for clarity
)
plt.title('Correlation Heatmap of Key Features', fontsize=14)  # Add a larger, clear title
plt.xticks(rotation=45, fontsize=10)  # Adjust x-axis labels for readability
plt.yticks(fontsize=10)  # Adjust y-axis labels for readability
plt.tight_layout()  # Ensure everything fits without overlap
plt.show()

"""Overall Quality has the strongest correlation with SalePrice (0.80), highlighting its significant influence on property values. Garage Area and Total Basement Square Footage also show relatively strong correlations with SalePrice, at 0.64 and 0.63, respectively, indicating that functional spaces play an important role in determining house prices.

Year Built shows a moderate correlation with SalePrice (0.56), suggesting that newer homes tend to be valued higher, although not as strongly as features like quality or space. On the other hand, Lot Area has the weakest correlation with SalePrice (0.27), meaning that lot size has a relatively minor impact on pricing in this dataset.

The heatmap also highlights some interesting relationships between the features themselves. For example, Overall Quality correlates well with Garage Area (0.56) and Total Basement SF (0.55), indicating that higher-quality homes are often associated with larger garages and basements. There is also a moderate correlation (0.49) between Garage Area and Total Basement SF, showing that these spaces often increase together.

### **Initial Visulization**
"""

numerical_features = ['SalePrice', 'Overall Qual', 'Garage Area', '1st Flr SF']
plt.figure(figsize=(12, 12))
sns.pairplot(df[numerical_features])
plt.suptitle('Scatter Plot Matrix of Key Features', y=1.02)
plt.show()

"""The pair plot above displays the relationships between key numerical features in the Ames housing dataset: SalePrice, Overall Quality (Overall Qual), Garage Area, and 1st Floor Square Footage (1st Flr SF). This visualization provides both scatter plots for pairwise relationships and histograms for the individual distributions of each variable, offering a detailed view of trends, patterns, and potential correlations.

The SalePrice distribution in the top-left histogram confirms that house prices are right-skewed, with the majority of properties priced between $100,000 and $300,000, while a smaller number of high-value properties extend beyond $600,000. This reinforces earlier observations that the Ames housing market is dominated by affordable and mid-range homes, with a few luxury properties as outliers.

When examining scatter plots, a strong positive correlation is observed between SalePrice and Overall Quality. Homes with higher quality ratings (8–10 on a scale of 1 to 10) are consistently associated with higher sale prices, showing a clear linear relationship. This underscores the importance of construction and material quality in determining property values. Conversely, homes with lower quality ratings (below 5) cluster within the lower price range.

The scatter plot between SalePrice and Garage Area also shows a positive relationship, though slightly less linear compared to Overall Qual. Larger garages, typically ranging between 400 and 1,000 square feet, are associated with higher sale prices, reflecting buyer preference for additional storage and functional space. However, some variability exists at the upper price range, indicating that other factors beyond garage size contribute to higher sale values.

Similarly, 1st Flr SF exhibits a strong positive correlation with SalePrice. Homes with larger first-floor areas (above 2,500 square feet) consistently command higher prices, reflecting the premium placed on spacious living areas. While the relationship is strong, some outliers appear in the upper end, indicating luxury properties with additional features that drive prices further upward.

The relationships between Overall Quality and other variables like Garage Area and 1st Flr SF are more discrete, as seen by the horizontal clustering of scatter points. This indicates that quality ratings are often categorical, with consistent ranges of Garage Area and 1st Floor Square Footage observed for each quality level.

The Garage Area and 1st Flr SF scatter plot, located in the lower-right, shows a moderate positive trend. Larger living areas are generally associated with larger garages, likely reflecting the typical preferences for spacious homes in suburban markets like Ames.
"""

plt.figure(figsize=(12, 6))
sns.violinplot(x='Overall Qual', y='SalePrice', data=df)
plt.title('Sale Price Distribution by Overall Quality')
plt.xlabel('Overall Quality')
plt.ylabel('Sale Price ($)')
plt.xticks(rotation=0)
plt.show()

"""The violin plot demonstrates a clear progression in house prices across quality ratings from 1 to 10. The shape of each "violin" represents the density of houses at different price points, with wider sections indicating more houses at that price level. Lower quality ratings (1-4) show concentrated distributions in the lower price range ($50,000-$150,000) with minimal variation. Middle-range qualities (5-7) display increasing price ranges and wider distributions, indicating more price variability. The highest quality ratings (8-10) exhibit the widest price ranges and highest values, reaching above $800,000, with quality 10 homes showing particularly high price concentrations in the upper ranges."""

plt.figure(figsize=(12, 6))
scatter = plt.scatter(df['1st Flr SF'], df['SalePrice'],
                      c=df['Overall Qual'], cmap='viridis',
                      alpha=0.6)
plt.colorbar(scatter, label='Overall Quality')
plt.title('Sale Price vs 1st Floor SF (colored by Overall Quality)')
plt.xlabel('1st Floor SF')
plt.ylabel('Sale Price ($)')
plt.show()

"""The scatter plot above depicts the relationship between Sale Price and 1st Floor Square Footage (1st Flr SF), with the data points color-coded by Overall Quality. This visualization highlights the interplay between floor space, sale price, and construction quality, offering insights into how these factors collectively influence housing prices.

The scatter plot reveals a positive correlation between 1st Floor SF and Sale Price. As the floor space increases, there is a clear upward trend in house prices. Homes with 1st Floor SF values between 1,000 and 2,500 square feet are the most common, with sale prices typically ranging from $100,000 to $400,000. A smaller number of properties with larger floor areas, exceeding 3,000 square feet, tend to reach significantly higher prices, surpassing $500,000. These larger homes are often indicative of luxury properties.

The color gradient, which represents Overall Quality, adds another layer of analysis. Higher-quality homes, denoted by lighter yellow-green shades (values of 8–10), are more prevalent in the upper price range. These homes consistently achieve higher sale prices, even when their floor space is moderate. For instance, homes with around 1,500 to 2,500 square feet but high-quality ratings (8–10) are priced similarly to larger homes with lower quality ratings (4–6). This highlights the significant influence of construction quality on property valuation, as buyers place a premium on well-constructed, high-quality homes.

At the lower end of the scale, homes with smaller floor areas (below 1,000 square feet) and lower quality ratings (values of 1–4) are clustered in the lower price range, typically below $150,000. These properties are less desirable, as they combine limited space with poor construction quality, resulting in lower market value.

Interestingly, the presence of outliers is notable, particularly in homes with extremely large floor spaces exceeding 4,000 square feet and high sale prices above $600,000. These outliers are almost exclusively associated with high-quality ratings (values of 9–10), reinforcing the role of quality in driving the upper end of the housing market.
"""

# Calculate the correlation coefficient
correlation = df['SalePrice'].corr(df['Garage Area'])
print(f"Correlation between Sale Price and Garage Area: {correlation:.2f}")

# Scatter plot to visualize the relationship
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Garage Area', y='SalePrice', data=df, alpha=0.6)
plt.title('Sale Price vs Garage Area', fontsize=14)
plt.xlabel('Garage Area (sq ft)', fontsize=12)
plt.ylabel('Sale Price ($)', fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

"""The scatter plot above illustrates the relationship between Sale Price and Garage Area in square feet for the Ames housing dataset. The correlation coefficient of 0.64 suggests a moderately strong positive relationship, indicating that houses with larger garage areas tend to have higher sale prices.

The plot reveals a concentration of data points around the 400–800 sq. ft. range for garage area, with corresponding sale prices between $100,000 and $300,000. This clustering reflects the typical garage size and price range for most houses in the dataset. As garage size increases beyond 800 sq. ft., sale prices generally rise, with some reaching over $500,000, suggesting that larger garages may be a feature of more expensive homes.

Notably, a few outliers are evident, such as properties with garage areas exceeding 1,200 sq. ft. or with very high sale prices that deviate from the general trend. These outliers likely represent luxury homes or properties with unique characteristics, such as oversized garages tailored to specific needs.

The graph highlights that while garage area significantly impacts sale price, it is not the sole determinant. Other features, such as overall quality or lot area, likely influence pricing and explain the spread of points within each garage size range. This insight underscores the need to consider multiple features in housing price prediction models.
"""

# Calculate the correlation coefficient
correlation_bsmt = df['SalePrice'].corr(df['Total Bsmt SF'])
print(f"Correlation between Sale Price and Total Basement Area: {correlation_bsmt:.2f}")

# Scatter plot to visualize the relationship
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Total Bsmt SF', y='SalePrice', data=df, alpha=0.6, color='teal')
plt.title('Sale Price vs Total Basement Area', fontsize=14)
plt.xlabel('Total Basement Area (sq ft)', fontsize=12)
plt.ylabel('Sale Price ($)', fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

"""The scatter plot illustrates the relationship between Sale Price and Total Basement Area in square feet for houses in the Ames housing dataset. The correlation coefficient of 0.63 indicates a moderately strong positive relationship, suggesting that larger basement areas are associated with higher house prices.

The majority of the data points are clustered between 500 and 2,000 sq. ft. for basement area, with corresponding sale prices ranging between $100,000 and $300,000. This clustering reflects typical basement sizes and price ranges for most houses in the dataset. Notably, as basement size increases beyond 2,000 sq. ft., sale prices generally rise, with some reaching over $500,000. This trend suggests that larger basement areas may be a feature of higher-end properties.

The plot also reveals a few outliers, such as properties with extremely large basements exceeding 4,000 sq. ft. or exceptionally high sale prices. These outliers likely represent unique or luxury properties, where the basement serves a specific purpose, such as entertainment spaces or additional living areas.

Interestingly, while the correlation is positive, there is noticeable dispersion in the data. For example, houses with basement areas of around 1,000 sq. ft. exhibit a wide range of sale prices, indicating that other factors, such as overall quality, location, or additional features, play a significant role in determining house prices. This insight highlights the importance of considering multiple features in predictive models to capture the complexity of the housing market.

"""

# Group by Neighborhood and calculate the average SalePrice
neighborhood_prices = df.groupby('Neighborhood')['SalePrice'].mean().sort_values()

# Plot the relationship
plt.figure(figsize=(12, 8))
sns.barplot(x=neighborhood_prices.index, y=neighborhood_prices.values, palette='viridis')
plt.title('Average Sale Price by Neighborhood', fontsize=16)
plt.xlabel('Neighborhood', fontsize=12)
plt.ylabel('Average Sale Price ($)', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""The bar chart highlights the average sale prices of houses across various neighborhoods in the Ames dataset, providing valuable insights into the disparity in property values. The data reveals a clear stratification among neighborhoods, with some areas commanding significantly higher prices than others. This variation reflects the influence of location, amenities, and overall housing quality on property values.

At the lower end of the spectrum, neighborhoods such as MeadowV, IDOTRR, and BrDale have the lowest average sale prices, indicating lower demand or possibly less desirable conditions. These areas may have smaller homes, fewer amenities, or other characteristics that make them less attractive to buyers. In contrast, premium neighborhoods like NoRidge, StoneBr, and NridgHt show average sale prices exceeding $300,000, making them the most expensive areas in the dataset. The higher prices in these neighborhoods could be attributed to larger homes, superior infrastructure, and proximity to desirable amenities.

The chart also highlights neighborhoods with mid-range average sale prices, such as Sawyer, Blueste, and Mitchel, which cluster around $150,000. These neighborhoods likely represent standard housing options that balance affordability with reasonable quality, catering to a broader segment of buyers.

The wide range of average sale prices, from approximately $70,000 in MeadowV to over $300,000 in NoRidge, underscores the diverse real estate landscape in Ames. This visualization not only illustrates the significant impact of location on property values but also provides a useful tool for identifying potential investment opportunities or affordable housing options within the dataset.
"""

df['Sale Date'] = pd.to_datetime(df['Yr Sold'].astype(str) + '-' + df['Mo Sold'].astype(str).str.zfill(2) + '-01')

# Set 'Sale Date' as the index
df.set_index('Sale Date', inplace=True)

# Resample data to monthly frequency and calculate mean price
monthly_prices = df['SalePrice'].resample('M').mean()

# Plot monthly average prices
plt.figure(figsize=(12, 6))
plt.plot(monthly_prices.index, monthly_prices.values)
plt.title('Monthly Average Sale Prices')
plt.xlabel('Date')
plt.ylabel('Average Sale Price')
plt.grid(True)
plt.show()

"""The line graph above illustrates the Monthly Average Sale Prices for residential properties in Ames, Iowa, spanning from early 2006 to mid-2010. This visualization highlights trends and fluctuations in property prices over time, offering insight into market dynamics across the analyzed period.

The graph shows considerable volatility in average sale prices, with several sharp peaks and troughs. Early in 2006, the average sale price began at a high point exceeding $200,000, but it experienced a significant decline shortly after, reaching approximately $160,000 mid-year. Following this dip, the market rebounded toward the end of 2006 and early 2007, achieving peaks near $220,000. This suggests periodic surges in housing demand or high-value sales during this time.

From 2007 to 2008, the average sale price fluctuates significantly, indicating a less stable market. Prices remain mostly between $170,000 and $200,000, but noticeable peaks, such as in early 2007 and late 2008, hint at short-term periods of higher activity or sales of high-priced properties. By mid-2008, a downward trend becomes more prominent, with average prices gradually declining, reaching a low around $160,000 at the start of 2009. This pattern aligns with the broader economic conditions during the global financial crisis, which likely impacted housing markets in Ames.

The years 2009 to 2010 reflect a general decline in average sale prices, interspersed with minor rebounds. The graph reveals another notable drop at the start of 2010, where prices briefly fell below $160,000, before recovering slightly. However, the downward trend becomes more pronounced in mid-2010, ending with a significant drop to approximately $145,000. This consistent decline suggests reduced housing demand, lower property values, or broader economic pressures affecting buyer activity.
"""

# Calculate year-over-year price changes
yoy_changes = monthly_prices.pct_change(12)

# Plot year-over-year price changes
plt.figure(figsize=(12, 6))
plt.plot(yoy_changes.index, yoy_changes.values)
plt.title('Year-over-Year Price Changes')
plt.xlabel('Date')
plt.ylabel('YoY Price Change (%)')
plt.grid(True)
plt.show()

"""The line chart above depicts the Year-over-Year (YoY) Price Changes in housing prices from 2007 to mid-2010, showing fluctuations in the percentage change of sale prices compared to the same month in the previous year. This chart is particularly useful for understanding trends in housing market performance over time and identifying periods of price growth or decline.

From 2007 to early 2008, the chart reveals intermittent periods of positive price growth interspersed with sharp declines. Notably, there is a significant positive spike around mid-2008, where YoY price changes exceed 0.2%. This suggests a temporary surge in property prices, possibly driven by localized market factors or increased buyer demand during that period. However, this growth is short-lived, as the trend dips back into negative territory towards the end of 2008.

Throughout 2009, the YoY price changes remain volatile, oscillating between -0.2% and +0.1%, indicating a period of market instability. While there are occasional upward movements, they are quickly offset by subsequent declines, reflecting the broader economic uncertainty during the aftermath of the 2008 financial crisis. This inconsistency suggests that while some buyers entered the market, general demand remained subdued, preventing sustained price recovery.

The final phase from late 2009 to mid-2010 shows a clear downward trend, culminating in a steep decline near -0.3% in mid-2010. This significant drop reflects a sharp decrease in housing prices compared to the same period in the prior year, likely indicating lingering economic challenges and reduced buyer confidence.
"""

# Analyze seasonality
seasonal_avg = df.groupby(df.index.month)['SalePrice'].mean()

plt.figure(figsize=(10, 6))
sns.barplot(x=seasonal_avg.index, y=seasonal_avg.values)
plt.title('Average Sale Price by Month')
plt.xlabel('Month')
plt.ylabel('Average Sale Price')
plt.show()

"""The bar chart above illustrates the Average Sale Price by Month for residential properties, providing insights into seasonal trends within the housing market. Each bar corresponds to the average sale price for a specific month, highlighting variations in housing prices over a typical year.

The data reveals that January (Month 1) records the highest average sale price, approaching $200,000. This suggests that early in the year, housing prices tend to peak, possibly driven by post-holiday buyer activity, limited inventory, or motivated sellers adjusting prices to capitalize on the new year. Following this, average prices experience a slight decline in February through April (Months 2–4), with prices dropping to around $170,000–$175,000. This reduction could reflect lower buyer activity during winter and early spring months, a common seasonal trend in real estate markets.

From May to August (Months 5–8), there is a notable recovery and stabilization in average sale prices, with values rising back to approximately $185,000–$190,000. These months coincide with the spring and summer seasons, which are typically periods of increased real estate activity. Families often prefer buying homes during this time to align with school schedules, while favorable weather conditions encourage property viewings and transactions.

The average sale price reaches its second peak in September (Month 9), again nearing $195,000. This suggests strong buyer demand persists into early fall, possibly driven by last-minute purchases before the holiday season. However, prices dip slightly in October (Month 10) to around $180,000, before rebounding in November and December (Months 11–12), stabilizing near $190,000.

# Modelling

# Baseline Model

To establish a baseline for performance comparison, a simple model was created to predict housing prices. This baseline model assumes that all houses sell for the same price, which is equal to the dataset's mean sale price. The model does not consider any property-specific features or variability within the data, making it a basic benchmark for evaluating the performance of more sophisticated models. By calculating the Mean Squared Error between the actual sale prices and the predicted mean value, we obtain a measure of error that represents how well the baseline model performs.
"""

# Set up baseline model using mean SalePrice
y = df['SalePrice']  # Target variable
baseline_preds = np.ones(len(y)) * y.mean()
mse = mean_squared_error(y, baseline_preds)

print(f"Baseline MSE: {mse}")

"""# Multiple Regression Model

In the refined approach, a Multiple Linear Regression Model was developed to improve on the baseline performance by leveraging key housing features that significantly influence sale prices. Feature selection was refined to include: Garage Cars, Garage Area, Total Basement Square Footage (Total Bsmt SF), 1st Floor Square Footage (1st Flr SF), Overall Quality (Overall Qual), and Neighborhood. These features were chosen based on their relevance to housing prices, representing a combination of physical attributes, quality indicators, and location factors.

The dataset was split into training and testing sets with an 80/20 split to evaluate model performance. This split ensures that the model is trained on a significant portion of the data while reserving a portion for testing its predictive capabilities on unseen data. By building a multiple linear regression model, the goal was to capture the independent contribution of each selected variable while accounting for their combined effects on sale prices.

The inclusion of both numerical and categorical features allows the model to handle a diverse range of property characteristics. Categorical variables, such as Neighborhood, were encoded appropriately to ensure they could be used in the regression model alongside numerical features. This updated model is designed to manage the complexity of the housing dataset, improve predictive performance, and address the shortcomings of the baseline model. The multiple regression model sets a foundation for further refinements using more advanced techniques to account for nonlinear relationships and complex interactions among variables.
"""

# Prepare features and target
X = df[['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual', 'Neighborhood']]
y = df['SalePrice']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)
    ])

# Evaluate the model
def evaluate_model(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"{model_name} Results:")
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R² Score: {r2:.4f}")

evaluate_model(y_test, linear_pred, "Linear Regression")

# Extract and display feature importance
def analyze_features(model, feature_names):
    # Get coefficients for numerical and one-hot encoded features
    coefficients = model.named_steps['regressor'].coef_
    feature_names_full = model.named_steps['preprocessor'].transformers_[0][2] + \
                         list(model.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out())

    importance_df = pd.DataFrame({
        'Feature': feature_names_full,
        'Importance': coefficients
    })
    return importance_df.sort_values('Importance', ascending=False)

# Get feature importance
feature_importance = analyze_features(linear_pipe, X.columns)
print("Feature Importance:")
print(feature_importance)

# Calculate MSE using the linear_pipe
train_predictions = linear_pipe.predict(X_train)
train_mse = mean_squared_error(y_train, train_predictions)

test_predictions = linear_pipe.predict(X_test)
test_mse = mean_squared_error(y_test, test_predictions)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")

# Extract the y-intercept
y_intercept = linear_pipe.named_steps['regressor'].intercept_
print(f"Y-intercept: {y_intercept:.2f}")

# Add cross-validation
cv_scores = cross_val_score(linear_pipe, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# Visualize predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Linear Regression: Predicted vs Actual House Prices')
plt.tight_layout()
plt.show()

"""The multiple regression model developed demonstrates robust predictive performance for housing prices in the Ames dataset. Achieving an R² score of 0.7936, the model explains approximately 79% of the variance in housing prices, signifying a substantial improvement over previous iterations. This strong R² value highlights the effectiveness of the selected features in capturing key determinants of property values.

The feature importance analysis sheds light on the drivers of housing prices. Overall Quality emerged as the most significant predictor, underlining the critical role of construction and finishing quality. Gr Liv Area followed as the second most influential feature, reflecting the importance of total living space in valuation. Meanwhile, Year Built ranked third, emphasizing the value associated with modern or newer constructions. Features such as Total Bsmt SF and Lot Area showed lower relative importance but still contributed to refining predictions. These results align with expectations in real estate markets, where quality and usable space often dominate pricing considerations.

The visualization of predicted versus actual prices supports these findings. The scatterplot shows a strong linear relationship between predictions and actual values, with most data points clustering near the diagonal, indicating accurate predictions. However, there is some deviation, particularly at higher price points, where the model tends to slightly underpredict or overpredict. This discrepancy might stem from outliers or unaccounted variables influencing premium properties.

Performance metrics further validate the model's reliability. The training MSE of 1,381,475,858.92 and testing MSE of 1,655,198,876.59 indicate a modest gap, suggesting the model generalizes well to unseen data. Cross-validation scores, averaging 0.7726, reinforce this consistency across different data splits, with scores ranging from 0.7024 to 0.8143, confirming the model's robustness.

### K - Nearest Neighbour Regression Model

The K-Nearest Neighbors (KNN) regression model was implemented to capture the influence of neighborhood effects and local market dynamics on housing prices. KNN is particularly effective for this task as it predicts prices by leveraging the similarity between houses with comparable features, such as size, location, and quality. Properties that share similar attributes tend to cluster together in price ranges, making KNN an intuitive and well-suited approach for this type of analysis.

This approach is especially advantageous when applied to the Ames housing dataset, as it emphasizes local price variations while accounting for neighborhood-specific characteristics. By analyzing key housing features such as Garage Area, Total Basement Square Footage (Total Bsmt SF), Lot Area, Overall Quality, and Year Built, the KNN model effectively identifies patterns in property valuation. This localized focus allows it to adapt predictions based on the immediate surroundings of a property, ensuring that neighborhood trends are reflected accurately.

To enhance performance and robustness, the KNN model incorporates cross-validation and hyperparameter tuning. These techniques optimize the number of neighbors (K) and the weight functions, ensuring that the model achieves a balance between bias and variance. Cross-validation validates the model's ability to generalize well on unseen data, while hyperparameter tuning fine-tunes its predictive accuracy.

This refined methodology not only delivers accurate predictions for housing prices but also effectively captures both local market trends and broader patterns across housing segments. As a result, the KNN model offers a reliable and interpretable tool for understanding price determinants within the Ames housing market, making it particularly useful for analyzing mid-sized housing markets and similar datasets.
"""

# Prepare features and target
X = df[['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual', 'Neighborhood']]
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_cols = ['Neighborhood']
num_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create ColumnTransformer with missing value handling
transformer = make_column_transformer(
    (Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Avoid unseen category errors
    ]), cat_cols),
    (Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ]), num_cols),
    remainder='passthrough'
)

# Test the transformer on X_train
transformed_X_train = transformer.fit_transform(X_train)
print("Transformed X_train shape:", transformed_X_train.shape)

# Create preprocessing pipeline
preprocessor = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Create KNN pipeline
knn_pipe = Pipeline([
    ('preprocessor', transformer),
    ('regressor', KNeighborsRegressor())
])

# Define parameter grid
param_grid = {
    'regressor__n_neighbors': [3, 5, 7, 9, 11, 25],
    'regressor__weights': ['uniform', 'distance']
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(knn_pipe, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Evaluate the model on test data
best_knn_model = grid_search.best_estimator_
knn_predictions = best_knn_model.predict(X_test)

# Calculate performance metrics
knn_mse = mean_squared_error(y_test, knn_predictions)
knn_rmse = np.sqrt(knn_mse)
knn_r2 = r2_score(y_test, knn_predictions)

print(f"KNN Best Parameters: {grid_search.best_params_}")
print(f"KNN MSE: {knn_mse:.2f}")
print(f"KNN RMSE: {knn_rmse:.2f}")
print(f"KNN R² Score: {knn_r2:.4f}")

# Perform 5-fold cross-validation
cv_scores = cross_val_score(knn_pipe, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# Calculate permutation importance
r = permutation_importance(best_knn_model, X_test, y_test, n_repeats=10, random_state=42)

# Extract feature names
categorical_features = ['Neighborhood']  # Represent Neighborhood as a single feature
feature_names = categorical_features + numeric_cols

# Aggregate importance for Neighborhood
neighborhood_indices = [i for i, name in enumerate(feature_names) if 'Neighborhood' in name]
neighborhood_importance = r['importances_mean'][neighborhood_indices].sum()

# Combine Neighborhood into one feature
importance_values = list(r['importances_mean'])
importance_values[len(categorical_features) - 1] = neighborhood_importance

# Create DataFrame of feature importances
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance_values
}).sort_values('Importance', ascending=False)

# Display feature importance
print("\nFeature Importance:")
print(feature_importance)

# Predict on the test set
test_predictions = best_knn_model.predict(X_test)

# Visualize predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('KNN: Predicted vs Actual House Prices')
plt.tight_layout()
plt.show()

"""The K-Nearest Neighbors (KNN) regression model shows a strong correlation between predicted and actual house prices in the Ames dataset, as illustrated in the graph. Most data points align closely with the red diagonal line, representing perfect predictions. This indicates that the model effectively captures the relationships between the selected features and housing prices, particularly for mid-range properties.

The model performs exceptionally well for properties within the mid-price range, where points are densely clustered around the diagonal. This suggests that the KNN model generalizes well for the majority of houses in the dataset, likely due to their greater representation in the training data. However, for higher-priced houses, deviations from the diagonal line are more apparent. Predicted prices for these properties often fall short of actual prices, reflecting potential difficulties in capturing the unique characteristics of high-value homes. These deviations could result from the limited number of training samples in this price range or the absence of features specific to luxury homes.

The minimal spread of residuals across the price range demonstrates the robustness of the model, avoiding significant overfitting or underfitting. The alignment of points across most price levels also highlights the effectiveness of the selected features, including Garage Area, Total Bsmt SF, Lot Area, Overall Quality, and Year Built. These features play a crucial role in determining housing prices, and their inclusion contributes significantly to the model's predictive power.

### Decision Tree Regression Model

The Decision Tree Regression model was implemented to analyze the Ames housing dataset due to its ability to effectively capture non-linear relationships while offering high interpretability. Unlike models such as K-Nearest Neighbors (KNN), which rely on feature similarity to predict housing prices, decision trees construct a hierarchical structure of decision rules that segment houses into distinct price ranges. This hierarchical nature makes decision trees particularly suitable for real estate price prediction, where sharp price transitions often occur based on specific feature thresholds, such as living area, year built, or basement size.

One of the key strengths of the decision tree model is its ability to automatically identify significant feature split points and thresholds. By detecting these nuanced interactions between housing characteristics, the model reveals how changes in features such as overall quality or square footage influence pricing. For example, the model can uncover distinct price patterns where slight increases in living space or construction quality lead to substantial shifts in housing values.

The decision tree’s inherent interpretability further enhances its value, as the model provides clear and transparent insights into which features drive housing prices. This makes it particularly useful for real estate professionals, homeowners, and market analysts, enabling them to make data-driven decisions based on an easily understandable model structure. By visualizing the decision tree, users can gain a deeper understanding of the underlying patterns within the dataset, facilitating more informed pricing strategies and actionable insights.
"""

# Prepare features and target
X = df[['Neighborhood', 'Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']]
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']  # Define 'Neighborhood' as categorical
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']

# Create column transformer
transformer = make_column_transformer(
    (OneHotEncoder(drop='first', sparse_output=False), cat_col),
    (StandardScaler(), numeric_cols),
    remainder='passthrough'
)

# Apply transformer to X_train and X_test
X_train_encoded = transformer.fit_transform(X_train)
X_test_encoded = transformer.transform(X_test)

# Find the optimal depth from test scores
test_scores = []
for depth in range(1, 11):  # Test depths from 1 to 10
    dtree = DecisionTreeRegressor(max_depth=depth, random_state=42)
    dtree.fit(X_train_encoded, y_train)
    test_scores.append(mean_squared_error(y_test, dtree.predict(X_test_encoded)))

optimal_depth = test_scores.index(min(test_scores)) + 1
print(f"Optimal Depth: {optimal_depth}")

# Train the decision tree with the optimal depth
dtree = DecisionTreeRegressor(max_depth=optimal_depth, random_state=42)
dtree.fit(X_train_encoded, y_train)

# Train the decision tree with a smaller depth
dtree = DecisionTreeRegressor(max_depth=4, random_state=42)
dtree.fit(X_train_encoded, y_train)

# Extract feature names
onehot_features = transformer.named_transformers_['onehotencoder'].get_feature_names_out(cat_col)
numeric_features = numeric_cols
feature_names = list(onehot_features) + numeric_features

# Plot the decision tree
plt.figure(figsize=(20, 8))
plot_tree(
    dtree,
    feature_names=feature_names,  # Use the corrected feature names here
    filled=True,
    fontsize=12
)
plt.title("Decision Tree (Max Depth = 4)", fontsize=16)
plt.tight_layout()
plt.show()

# Calculate MSE for both sets
train_preds = dtree.predict(X_train_encoded)
test_preds = dtree.predict(X_test_encoded)

train_mse = mean_squared_error(y_train, train_preds)
test_mse = mean_squared_error(y_test, test_preds)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")

# Calculate additional metrics
train_r2 = r2_score(y_train, train_preds)
test_r2 = r2_score(y_test, test_preds)
rmse = np.sqrt(test_mse)

print(f"Training R²: {train_r2:.4f}")
print(f"Testing R²: {test_r2:.4f}")
print(f"RMSE: {rmse:.2f}")

# Perform cross-validation
cv_scores = cross_val_score(dtree, X_train_encoded, y_train, cv=5, scoring='r2')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# Convert sparse to dense array if necessary
X_test_dense = X_test_encoded.toarray() if hasattr(X_test_encoded, 'toarray') else X_test_encoded

r = permutation_importance(dtree, X_test_dense, y_test, n_repeats=10, random_state=42)

# Extract feature names
feature_names = transformer.named_transformers_['onehotencoder'].get_feature_names_out(cat_col)
all_feature_names = list(feature_names) + numeric_cols

# Create DataFrame for feature importance
importance_df = pd.DataFrame({
    'Feature': all_feature_names,
    'Importance': r['importances_mean']
}).sort_values(by='Importance', ascending=False)

# Print and visualize results
print("\nFeature Importance:")
print(importance_df)

# Plot residuals
residuals = y_test - test_predictions
plt.figure(figsize=(10, 6))
plt.scatter(test_predictions, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

"""The residual plot shown above visualizes the relationship between the predicted values and the residuals (the difference between observed and predicted values) for the model's predictions. Each point on the graph represents an individual prediction, with the x-axis displaying the predicted values and the y-axis showing the corresponding residuals. A residual value of zero (represented by the red dashed line) indicates that the model's prediction perfectly matches the observed value, while points above or below this line signify over-prediction and under-prediction, respectively.

The spread of the residuals appears to vary across the range of predicted values. For lower predicted values (closer to 100,000 to 200,000), the residuals are relatively tightly clustered around zero, indicating that the model performs reasonably well in this range. However, there is still some moderate vertical dispersion, reflecting occasional errors in the predictions. As the predicted values increase beyond 300,000, the residuals show greater variability, with a few points exhibiting significant deviations from the zero line, suggesting larger prediction errors. These outliers might indicate specific instances where the model struggles to capture certain high-value properties.

The residuals do not appear to be randomly distributed, as certain patterns can be observed in the vertical clustering of points at distinct predicted values. This non-random distribution may indicate the presence of systematic biases in the model, where it consistently overestimates or underestimates prices for certain property ranges. For instance, the residuals for higher predicted values (above 350,000) show more pronounced deviations both above and below zero, signaling greater difficulty in predicting high-value homes.

### Random Forest Regression Model

I chose to implement a Random Forest regression model for the Ames housing dataset due to its robustness and versatility in handling complex relationships and feature interactions. Unlike single decision trees, which are prone to overfitting, Random Forests aggregate predictions from multiple decision trees, creating an ensemble model that reduces variance while maintaining the interpretability of decision tree-based models. This ensemble approach allows the model to capture non-linear patterns and interactions between housing features, which are crucial in understanding housing price dynamics.

Random Forests are particularly well-suited for this dataset as they excel in managing high-dimensional data, handling both categorical and continuous variables seamlessly. By randomly selecting subsets of features and samples at each split, the model avoids over-reliance on any single feature or data point, ensuring more generalized predictions. This is especially valuable in real estate price prediction, where factors like quality, size, location, and year built interact in complex, non-linear ways.

Additionally, the model provides feature importance metrics, offering insights into the most influential predictors in determining housing prices. This transparency not only enhances interpretability but also helps prioritize key attributes that significantly impact pricing strategies. Given its balance between predictive accuracy, feature interpretability, and robustness to overfitting, Random Forest is a powerful model for addressing the challenges of housing price prediction in the Ames dataset.
"""

# Prepare features and target
X = df[['Neighborhood', 'Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']]
y = df['SalePrice']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical columns
cat_col = ['Neighborhood']  # Define 'Neighborhood' as categorical
numeric_cols = ['Garage Cars', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Overall Qual']


preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', X_train.select_dtypes(include=['int64', 'float64']).columns),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), non_numeric_columns)
    ])

# Create pipeline for random forest model
pipe = Pipeline([
    ('preprocessor', preprocessor),
    ('model', estimator)  # Your original estimator
])


# Define grid of hyperparameters
param_grid = {
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10]
}

estimator = RandomForestRegressor(random_state=42)

non_numeric_columns = X_train.select_dtypes(include=['object', 'category']).columns
print("Non-numeric columns:", non_numeric_columns)

# Perform grid search with cross-validation
grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', error_score='raise')
grid_search.fit(X_train, y_train)

# Get best parameters
print("Best parameters:", grid_search.best_params_)

# Use the best model from grid search
forest = grid_search.best_estimator_

# Make predictions
train_preds = forest.predict(X_train)
test_preds = forest.predict(X_test)

# Calculate performance metrics
train_mse = mean_squared_error(y_train, train_preds)
test_mse = mean_squared_error(y_test, test_preds)
r2_score_val = r2_score(y_test, test_preds)

print(f"Training MSE: {train_mse:.2f}")
print(f"Testing MSE: {test_mse:.2f}")
print(f"R2 Score: {r2_score_val:.4f}")

# Calculate feature importance using permutation importance
r = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42)
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': r['importances_mean']
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
print(importance_df)

transformer = make_column_transformer(
    (OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_col),
    (StandardScaler(), numeric_cols),
    remainder='passthrough'
)

# Cross-validation scores display
cv_scores = cross_val_score(forest, X_train, y_train, cv=5)
print(f"Cross-validation scores: {cv_scores}")
print(f"Average CV score: {cv_scores.mean():.4f}")

# First make predictions
test_predictions = forest.predict(X_test)

# Then create visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Random Forest: Predicted vs Actual House Prices')
plt.show()

"""The scatter plot above visualizes the performance of the Random Forest regression model by comparing predicted house prices against their actual values. The red dashed line represents the ideal scenario where predictions perfectly match the actual values, and data points lie on the line.

In this plot, most predictions cluster closely around the diagonal line, particularly for houses in the mid-price range. This indicates that the Random Forest model is performing well for properties within the average pricing brackets. However, the dispersion of points increases at higher price ranges, showing that the model struggles slightly with more expensive homes, where residuals (errors) are larger. This behavior could be due to the limited representation of higher-priced properties in the dataset, leading the model to generalize less effectively for these instances.

Additionally, the plot shows no obvious systematic bias in the predictions, as the spread of points is relatively even across the price range. This suggests that the model does not consistently overestimate or underestimate prices at specific intervals.

Overall, the Random Forest model demonstrates strong predictive accuracy, as evidenced by the tight clustering around the diagonal for most points. While it underperforms slightly for luxury properties, its ability to capture general price trends across the dataset confirms its utility as a robust and reliable model for predicting housing prices. This analysis aligns with the model's capability to aggregate decisions from multiple trees, reducing variance and improving overall predictive performance.
"""